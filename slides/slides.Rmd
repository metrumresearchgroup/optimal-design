---
title: "A Gentle Introduction to Optimal Design for Pharmacometric Models"
subtitle: "with PopED and mrgsolve"
author: "Tim Waterhouse and Kyle Baron"
institute: "Metrum Research Group"
date: "8 June, 2020"
output:
  xaringan::moon_reader:
    css: ["tweaks.css", "default", "default-fonts"]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
    seal: false
---

```{r setup, include=FALSE}
#options(htmltools.dir.version = FALSE)
knitr:::opts_chunk$set(comment = '.', message = FALSE, warning = FALSE)

#class: title-slide   
#
## A Gentle Introduction to Optimal Design <br/> for Pharmacometric Models
### with PopED and mrgsolve
#
### Tim Waterhouse and Kyle Baron <br/> Metrum Research Group
### 8 June, 2020
#
#---
```


layout: true

<div class="my-footer"></div>       

---

background-image: linear-gradient(90deg, rgba(255,255,255,1) 50%, rgba(0,0,0,0) 70%), url(https://live.staticflickr.com/4422/36449778360_8683e16f7f_k_d.jpg)
background-size: cover
  
# Why listen to this talk?

.pull-left[
* You want to design a study
{{content}}
]

--

* You'll be fitting a model to the results
{{content}}

--

* You don't want that model to fail spectacularly
{{content}}

--

* You don't have the time or the patience to run a bunch of simulations
{{content}}


???

Image credit: [Flickr](https://www.flickr.com/photos/18378305@N00/36449778360)

---

background-image: linear-gradient(90deg, rgba(255,255,255,1) 50%, rgba(0,0,0,0) 70%), url(https://live.staticflickr.com/35/70342823_cd9f9ec744_h_d.jpg)
background-size: cover
  
# Outline

* Optimal design background
* Software tools

???

Image credit: [Flickr](https://www.flickr.com/photos/davidden/70342823/)
---
class: inverse, center, middle

# Optimal design background

---

# Meet the Fisher information matrix

.pull-left[
$$ M_F(\mathbf{\Psi},\xi)
= - \operatorname{E}
\left[\left.
 \frac{\partial^2}{\partial\mathbf{\Psi} \partial\mathbf{\Psi}^T} \log L(\mathbf{\Psi};y)
\right|\mathbf{\Psi}\right] $$
]

.pull-right[
where

* $\mathbf{\Psi}$ is the vector of populations parameters (e.g. `THETA`s, `OMEGA`s, and `SIGMA`s in NONMEM),
* $y$ is the vector of observations,
* $\xi$ is the vector of design variables (e.g. sampling times), and
* $\log L$ is the log-likelihood.
]

---

# Why should I care about that thing?

[Cram√©r-Rao  lower bound](https://en.wikipedia.org/wiki/Cram%C3%A9r%E2%80%93Rao_bound):

$$
\mathrm{cov}(\hat{\mathbf{\Psi}}) \geq  M_F(\mathbf{\Psi},\xi)
$$

when $\hat{\mathbf{\Psi}}$ is an unbiased estimator of $\mathbf{\Psi}$.

--

This means we have a quick way of evaluating (a lower bound on) the precision of our parameter estimates.

--

.center[![](https://emojipedia-us.s3.dualstack.us-west-1.amazonaws.com/thumbs/240/apple/237/smiling-face-with-sunglasses_1f60e.png)]

???

As the name implies, optimal designs are experimental designs that are optimal with respect to some criterion.  Many such criteria exist, but most involve the [Fisher information matrix](https://en.wikipedia.org/wiki/Fisher_information) (FIM).  This matrix is useful because it gives us a [lower bound](https://en.wikipedia.org/wiki/Cram%C3%A9r%E2%80%93Rao_bound) on the covariance matrix of our parameter estimates.


---

# OK, but really. Why should I care about that thing?

.pull-left[
* $D$-optimality criterion


* blah
]

--

.pull-right[

```{r echo=F}
knitr::include_graphics("pics/confidence_ellipsoid.svg")
```


]

???

The most commonly used criterion is the *D*-optimalilty criterion.  *D*-optimal designs maximize the determinant of the FIM, which is equivalent to minimizing the (lower bound of) the determinant of the covariance matrix of the parameter estimates.  For a single parameter, this means we're minimizing the width of its confidence interval, estimating it as precisely as possible.  Extending this to multiple parameters, we're minimizing the volume of the confidence ellipsoid, which loosely translates to maximizing the overall precision of parameter estimates.

The FIM is typically notated by something like *M*<sub>*F*</sub>(&Phi;,&Xi;), where &Phi; is the vector of parameter values (e.g. *CL*, &omega;<sub>*CL*</sub>, etc.) and &Xi; is the vector of design variables (e.g. dose levels, PK sampling times, etc.).  For linear models, the dependence on the parameters disappears.  Unfortunately for us, this is not the case for nonlinear models.  So in order to design our experiment in a way that will produce the best parameter estimates, we first need to know the values of those parameters.  This is the catch-22 of optimal design.  The good news is that we usually have *some* sense of parameter values from earlier clinical trials or even predictions from animal data.  We can even incorporate uncertainty of the estimates (e.g. with *ED*- or *HC*ln*D*-optimality).

---

# Nonlinear mixed effects models

More often than not, we're dealing with nonlinear mixed effects (NLME) models.  Since the FIM depends on the likelihood function, and there is sadly no analytic expression for the likelihood in NLME models, we must rely on approximations.  See Mentre1997-ds, Retout2001-lw, and Retout2003-jx for FIM approximations available to us.

So our FIM is

* an approximation
* to a lower bound
* that depends on the parameter values.

How useful could that even be?  Pretty useful, actually.  In most cases you'll probably find that you have adequate information on parameter estimates and that these approximate lower bounds aren't too far off what you'll get from simulations.

Either way, I **strongly** recommend that any optimal design exercise is capped off with confirmatory simulations using the tool (e.g. NONMEM) that you'll be using in the actual analysis of the data.

---

# Evaluation vs Optimization

Optimal design tools can be used in the way that the name implies (to actually optimize a study), or we can simply evaluate a design with a quick calculation of the FIM (and a translation of the FIM to expected relative standard errors).  Optimization is usually a "last resort", and most of the time you'll only need to evaluate a few potential designs before settling on the answer.

That's not to say that optimization doesn't have its place.  For example, resources may be very tightly constrained, or intuitive selection of potential doses or sampling times doesn't produce sufficient results.  In these cases, we'd use a search algorithm to determine an optimal design.

---

# Sampling windows

A common application of optimal design is the selection of PK sampling times.  In practice, we often can't practically collect PK samples at precise times.  Also, optimization of sampling times will usually tell you to collect samples at seemingly bizarre times like 3.4756 hours.  Or worse, it may require *multiple* samples at the same time (if you can't specify a minimum period between samples).  In these cases, we can specify windows of time for each collection.

Although methods exist for optimal determination of the windows, you will mostly likely be able to do a perfectly good job yourself by picking these manually.  You can then determine the suitability of the windows by seeing how uniform sampling within the windows impacts relative standard errors (both in optimal design tools like `PopED` or in your simulation).


---
class: inverse, center, middle

# Example: closed-from PK model

---
class: inverse, center, middle

# Example: ODE PK model

---
class: inverse, center, middle

# Example: ODE PK/PD model

---

https://www.flickr.com/photos/msbhaven/7404593082
https://www.flickr.com/photos/tinahesmansaey/2599260050
https://www.flickr.com/photos/hyperxp/6847495120
https://www.flickr.com/photos/dmuth/6039404494
https://www.flickr.com/photos/dmuth/6039404304

https://www.flickr.com/photos/dok1/3581901453
https://www.flickr.com/photos/bucky1105/2334669404
https://www.flickr.com/photos/lalonsorm/16856604020
https://www.flickr.com/photos/gmanviz/48835009102
