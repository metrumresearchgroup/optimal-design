---
title: "A Gentle Introduction to Optimal Design for Pharmacometric Models"
subtitle: "with PopED and mrgsolve"
author: "Tim Waterhouse and Kyle Baron"
institute: "Metrum Research Group"
date: "8 June, 2020"
output:
  xaringan::moon_reader:
    css: ["tweaks.css", "default", "default-fonts"]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
    seal: false
---

```{r setup, include=FALSE}
#options(htmltools.dir.version = FALSE)
knitr:::opts_chunk$set(
  comment = '.', message = FALSE, warning = FALSE, fig.retina = 3
)
library(tidyverse)
library(PopED)
library(glue)
```

```{r, load_refs, include=FALSE, cache=FALSE}
library(RefManageR)
BibOptions(
  check.entries = FALSE,
  bib.style = "authoryear",
  cite.style = "authoryear",
  style = "markdown",
  hyperlink = FALSE,
  dashed = FALSE
)
myBib <- ReadBib("./refs.bib", check = FALSE)
```

class: title-slide   

# A Gentle Introduction to Optimal Design <br/> for Pharmacometric Models
## with PopED and mrgsolve

## Tim Waterhouse and Kyle Baron <br/> Metrum Research Group
## 8 June, 2020

---


layout: true

<div class="my-footer"></div>       

---

background-image: linear-gradient(90deg, rgba(255,255,255,1) 50%, rgba(0,0,0,0) 70%), url(https://live.staticflickr.com/4422/36449778360_8683e16f7f_k_d.jpg)
background-size: cover
  
# Why listen to this talk?

.pull-left[
* You want to design a study
{{content}}
]

--

* You'll be fitting a model to the results
{{content}}

--

* You don't want that model to fail spectacularly
{{content}}

--

* You don't have the time or the patience to run a bunch of simulations
{{content}}


???

Image credit: [Flickr](https://www.flickr.com/photos/18378305@N00/36449778360)

---

background-image: linear-gradient(90deg, rgba(255,255,255,1) 50%, rgba(0,0,0,0) 70%), url(https://live.staticflickr.com/35/70342823_cd9f9ec744_h_d.jpg)
background-size: cover
  
# Outline

* Optimal design background

* Software tools

???

Image credit: [Flickr](https://www.flickr.com/photos/davidden/70342823/)

---

class: poll-slide

# Poll Question #1

## What is your experience with optimal design?

1. Never heard of it.

--

2. I've seen it around, and I'd like to try it.

--

3. I've seen it around, but it seems kinda fishy.

--

4. I've used optimal design for some studies.

--

5. I invert Fisher information matrices in my head.

--

6. Huh? Sorry, I was checking email.
---

class: inverse, center, middle

# Optimal design background

---

# Meet the Fisher information matrix (FIM)

.pull-left[
$$ M_F(\mathbf{\Psi},\xi)
= - \operatorname{E}
\left[\left.
 \frac{\partial^2}{\partial\mathbf{\Psi} \partial\mathbf{\Psi}^T} \log L(\mathbf{\Psi};y)
\right|\mathbf{\Psi}\right] $$

where

* $\mathbf{\Psi}$ is the vector of populations parameters (e.g. `THETA`s, `OMEGA`s, and `SIGMA`s in NONMEM),

* $y$ is the vector of observations,

* $\xi$ is the vector of design variables (e.g. sampling times), and

* $\log L$ is the log-likelihood.
]

.pull-right[
```{r echo=FALSE}
knitr::include_graphics("pics/Fisher1946.jpeg")
```
]

---

# Why should I care about that thing?

[Cram√©r-Rao  lower bound](https://en.wikipedia.org/wiki/Cram%C3%A9r%E2%80%93Rao_bound):

$$
\mathrm{cov}(\hat{\mathbf{\Psi}}) \geq  \left[M_F(\mathbf{\Psi},\xi)\right]^{-1}
$$

when $\hat{\mathbf{\Psi}}$ is an unbiased estimator of $\mathbf{\Psi}$.

--

* Relative standard errors (RSEs) can be obtained from the diagonals of the inverse of the FIM

--

* This means we have a quick way of evaluating (a lower bound on) the precision of our parameter estimates.

--

.center[![](https://emojipedia-us.s3.dualstack.us-west-1.amazonaws.com/thumbs/240/apple/237/smiling-face-with-sunglasses_1f60e.png)]

???

As the name implies, optimal designs are experimental designs that are optimal with respect to some criterion.  Many such criteria exist, but most involve the [Fisher information matrix](https://en.wikipedia.org/wiki/Fisher_information) (FIM).  This matrix is useful because it gives us a [lower bound](https://en.wikipedia.org/wiki/Cram%C3%A9r%E2%80%93Rao_bound) on the covariance matrix of our parameter estimates.


---

# OK, but really. Why should I care about that thing?

.pull-left[
$$
\mathrm{cov}(\hat{\mathbf{\Psi}}) \geq  M_F(\mathbf{\Psi},\xi)
$$
]

.pull-right[
* $D$-optimality criterion
{{content}}
]

--

* $D$-optimal designs maximise the determinant of the FIM
{{content}}

--

* Equivalent to minimising the volume of the confidence ellipsoid of the parameter estimates
{{content}}

--

* Huh?
{{content}}

???

The most commonly used criterion is the *D*-optimalilty criterion.  *D*-optimal designs maximize the determinant of the FIM, which is equivalent to minimizing the (lower bound of) the determinant of the covariance matrix of the parameter estimates.  For a single parameter, this means we're minimizing the width of its confidence interval, estimating it as precisely as possible.  Extending this to multiple parameters, we're minimizing the volume of the confidence ellipsoid, which loosely translates to maximizing the overall precision of parameter estimates.

---

# This is a confidence ellipsoid in 2 dimensions

.center[
```{r echo=F, out.width='50%'}
knitr::include_graphics("pics/confidence_ellipsoid.svg")
```
]

---

background-image: linear-gradient(90deg, rgba(255,255,255,1) 50%, rgba(0,0,0,0) 70%), url(https://live.staticflickr.com/7241/7404593082_9877908077_k_d.jpg)
background-size: cover
  
# Catch-22 of optimal design

.pull-left[
* For linear models, the dependence of $M_F(\mathbf{\Psi},\xi)$ on $\mathbf{\Psi}$ disappears
{{content}}
]

--

* No such luck for nonlinear models
{{content}}

--

* In order to design our experiment in a way that will produce the best parameter estimates, we first need to know the values of those parameters
{{content}}

--

```{r echo=F, out.width='30%'}
knitr::include_graphics("https://cdn.shopify.com/s/files/1/1061/1924/products/7_large.png?v=1571606116")
```
{{content}}

???

The FIM is typically notated by something like *M*<sub>*F*</sub>(&Phi;,&Xi;), where &Phi; is the vector of parameter values (e.g. *CL*, &omega;<sub>*CL*</sub>, etc.) and &Xi; is the vector of design variables (e.g. dose levels, PK sampling times, etc.).  For linear models, the dependence on the parameters disappears.  Unfortunately for us, this is not the case for nonlinear models.  So in order to design our experiment in a way that will produce the best parameter estimates, we first need to know the values of those parameters.  This is the catch-22 of optimal design.  The good news is that we usually have *some* sense of parameter values from earlier clinical trials or even predictions from animal data.  We can even incorporate uncertainty of the estimates (e.g. with *ED*- or *HC*ln*D*-optimality).

Image credit: [Flickr](https://www.flickr.com/photos/msbhaven/7404593082)

---

# Nonlinear mixed effects models are even more problematic

.pull-left[
* No analytic expression for the likelihood, so we rely on approximations

* So our FIM is

    * an approximation

    * to a lower bound

    * that depends on the parameter values
]

.footnote[`r Citet(myBib, "Mentre1997-ds")`</br>
`r Citet(myBib, "Retout2001-lw")`</br>
`r Citet(myBib, "Retout2003-jx")`]

???

More often than not, we're dealing with nonlinear mixed effects (NLME) models.  Since the FIM depends on the likelihood function, and there is sadly no analytic expression for the likelihood in NLME models, we must rely on approximations.  See Mentre1997-ds, Retout2001-lw, and Retout2003-jx for FIM approximations available to us.

So our FIM is

* an approximation
* to a lower bound
* that depends on the parameter values.

---

background-image: linear-gradient(90deg, rgba(255,255,255,1) 50%, rgba(0,0,0,0) 70%), url(https://live.staticflickr.com/3061/2599260050_539b8904c7_k_d.jpg)
background-size: cover
  
# All is not lost

.pull-left[
* Usually we have adequate information on parameters estimates
{{content}}
]

--

* Approximate lower bounds are usually not far off from values obtained from simulation
{{content}}

--

* More to come on simulation...
{{content}}

???

How useful could that even be?  Pretty useful, actually.  In most cases you'll probably find that you have adequate information on parameter estimates and that these approximate lower bounds aren't too far off what you'll get from simulations.

Either way, I **strongly** recommend that any optimal design exercise is capped off with confirmatory simulations using the tool (e.g. NONMEM) that you'll be using in the actual analysis of the data.

Image credit: [Flickr](https://www.flickr.com/photos/tinahesmansaey/2599260050)

---

background-image: linear-gradient(90deg, rgba(255,255,255,1) 50%, rgba(0,0,0,0) 70%), url(https://live.staticflickr.com/7049/6847495120_b74032750a_k_d.jpg)
background-size: cover
  
# Evaluation vs Optimisation

.pull-left[
* Optimal design can be used to optimise a study (duh)
{{content}}
]

--

* We can also just use the FIM to quickly evaluate a design by calculating RSEs
{{content}}

--

* Optimisation is often a last resort (we can just evaluate a few candidate designs in many situations)
{{content}}

--

* Sometimes resources are too tightly constrained or our intuition isn't good enough to find feasible designs without optimising using a search algorithm
{{content}}

???

Optimal design tools can be used in the way that the name implies (to actually optimize a study), or we can simply evaluate a design with a quick calculation of the FIM (and a translation of the FIM to expected relative standard errors).  Optimization is usually a "last resort", and most of the time you'll only need to evaluate a few potential designs before settling on the answer.

That's not to say that optimization doesn't have its place.  For example, resources may be very tightly constrained, or intuitive selection of potential doses or sampling times doesn't produce sufficient results.  In these cases, we'd use a search algorithm to determine an optimal design.

Image credit: [Flickr](https://www.flickr.com/photos/hyperxp/6847495120)

---

# Sampling windows

.pull-left[
```{r echo=FALSE, out.width='90%', fig.width=3, fig.height=3}
windows1 <- tibble(x = c(1, 2))
windows2 <- tibble(x = c(8, 10))
windows3 <- tibble(x = c(22, 24))
tibble(x = seq(0, 24, length = 50)) %>% 
  mutate(y = (10000 * 0.25/(100 * (0.25 - 10/100))) *
      (exp(-10/100 * x) - exp(-0.25 * x))) %>% 
  ggplot() +
  geom_ribbon(aes(x = x, ymin = 0, ymax = 60), data = windows1,
              fill = "lightblue") +
  geom_ribbon(aes(x = x, ymin = 0, ymax = 60), data = windows2,
              fill = "lightblue") +
  geom_ribbon(aes(x = x, ymin = 0, ymax = 60), data = windows3,
              fill = "lightblue") +
  geom_line(aes(x, y)) +
  labs(x = "Time", y = "Concentration") +
  scale_x_continuous(breaks = seq(0, 24, by = 6)) +
  theme_bw()
```
]

.pull-right[
* "Optimal" sampling times are often not practical
{{content}}
]

--

* Even without optimisation, we can't always collect samples at precise times
{{content}}

--

* Sampling windows can be optimised or determined manually
{{content}}

???

A common application of optimal design is the selection of PK sampling times.  In practice, we often can't practically collect PK samples at precise times.  Also, optimization of sampling times will usually tell you to collect samples at seemingly bizarre times like 3.4756 hours.  Or worse, it may require *multiple* samples at the same time (if you can't specify a minimum period between samples).  In these cases, we can specify windows of time for each collection.

Although methods exist for optimal determination of the windows, you will mostly likely be able to do a perfectly good job yourself by picking these manually.  You can then determine the suitability of the windows by seeing how uniform sampling within the windows impacts relative standard errors (both in optimal design tools like `PopED` or in your simulation).


---

# Tools for optimal design

```{r echo=F}
knitr::include_graphics("pics/tools.png")
```

.footnote[`r Citet(myBib, "Nyberg2014-nk")`]

---

# PopED

.pull-left[
```{r echo=F}
knitr::include_graphics("pics/PopED_logo.png")
```
]

.pull-right[
* https://andrewhooker.github.io/PopED/

* Originally in O-Matrix and Matlab, now an R package
]
.footnote[`r Citet(myBib, "Foracchia2004-yc")` <br>
`r Citet(myBib, "Nyberg2012-gd")`]

---

# SSE: Stochastic Simulation and Estimation

.center[
```{r echo=F, out.width='65%'}
knitr::include_graphics("pics/sse.svg")
```
]

---

class: inverse, center, middle

# Example: Closed-form PK model

---

# Introducing our example

.pull-left[
**Mockdrozaline** has been studied in adult subjects, and we now must design a study in pediatric patients.

The primary objective is to evaluate the PK in this new population, but PK sampling is necessarily sparse.

Our mission is to ensure that these samples are timed such that we can sufficiently estimate the PK parameters in pediatric patients.
]

--

.pull-right[

* Population

  - 12 subjects
  - Aged 6 to < 12
  - Expected median weight of 32 kg

* Treatment

  - 10 mg QD mockdrozaline for 24 weeks

* PK samples

  - Proposed samples:
  
      + 5 hours postdose on Day 1;
      + predose on Weeks 8, 12, 24; and
      + 168 hours after the final dose


]
???

Mockdrozaline has been studied in adult subjects, and we now must design a study in pediatric patients.  The primary objective is to evaluate the PK in this new population, but PK sampling is necessarily sparse.  Our mission is to ensure that these samples are timed such that we can sufficiently estimate the PK parameters in pediatric patients.

---

# The model

.pull-left[
1-compartment model with 1st-order absorption and weight covariates on CL and V:
```{r echo=FALSE}
tibble(
  CL = 10, V = 100, KA = 0.25,
  wt_cl = 0.75, wt_v = 1
) %>% 
  knitr::kable(format = "html")
```

Log-normal IIV on `CL`, `V`, and `KA`; additive & proportional residual error:
```{r echo=FALSE}
tibble(
  om_CL = 0.08, om_V = 0.1, om_KA = 0.2, sigma_prop = 0.05, sigma_add = 1
) %>% 
  knitr::kable(format = "html")
```
(these are variances)
]

--

.pull-right[
```{r echo=FALSE, out.width='90%', fig.width=3, fig.height=3}
pk <- function(xt, dose, dose_times){
  CL <- 10
  V <- 100
  KA <- 0.25
  
  y <- rep(0, length(xt))
  for (i in seq_along(dose_times)) {
    y <- y + (dose * KA/(V * (KA - CL/V))) *
      (exp(-CL/V * pmax(xt - dose_times[i], 0)) -
         exp(-KA * pmax(xt - dose_times[i], 0)))
  }
  return(y)
}
n_doses <- 4
tibble(xt = seq(0, n_doses*24, by = 1)) %>% 
  mutate(y = pk(xt, dose = 1e4, dose_times = seq(0, n_doses*24, by = 24))) %>% 
  ggplot(aes(xt, y)) +
  geom_line() +
  labs(x = "Time (hr)", y = "Mockdrozaline Concentration (ng/mL)") +
  theme_bw()
```
]

???

A wealth of data in adults has allowed us to describe orally-administered mockdrozaline PK using a 1-compartment model with first-order absorption, and 2 covariates: weight on clearance (`wt_cl`) and weight on volume (`wt_v`).

We include log-normal IIV on `CL`, `V`, and `KA`, and a combined additive and proportional residual error.

Note that we reach steady state very quickly (the half-life is around 7 hours), so we can assume that all samples after Day 1 (i.e. from Week 4 onward) are at steady state.

---

class: poll-slide

# Poll Question #2

## Which of these designs will give us the best<sup>*</sup> RSE for KA, assuming a single 10 mg dose in 20 adult (70 kg) subjects?

```{r echo=FALSE}
designs <- bind_cols(
  `Design 1` = c(1, 24, 48, 96),
  `Design 2` = c(4, 24, 48, 96),
  `Design 3` = c(6, 24, 48, 96),
  `Design 4` = c(12, 24, 48, 96)
) %>% 
  gather("design", "xt") %>% 
  mutate(y = pk(xt, dose = 1e4, dose_times = 0))
```

.pull-left[
1. `r designs %>% dplyr::filter(design == "Design 1") %>% pull(xt)` hours

2. `r designs %>% dplyr::filter(design == "Design 2") %>% pull(xt)` hours

3. `r designs %>% dplyr::filter(design == "Design 3") %>% pull(xt)` hours

4. `r designs %>% dplyr::filter(design == "Design 4") %>% pull(xt)` hours

$t_\textrm{max} \approx$ 6 hours
]

.pull-right[
```{r echo=FALSE, out.width='90%', fig.width=5, fig.height=3}
tibble(xt = seq(0, 96, by = 1)) %>% 
  mutate(y = pk(xt, dose = 1e4, dose_times = 0)) %>% 
  ggplot(aes(xt, y)) +
  geom_line() +
  geom_point(aes(colour = design, pch = design), data = designs) +
  scale_x_continuous(breaks = seq(0, 96, by = 24)) +
  scale_y_log10(limits = c(1e-2, 100)) +
  #scale_y_log10() +
  labs(x = "Time (hr)", y = "Mockdrozaline Concentration (ng/mL)",
       colour = NULL, pch = NULL) +
  theme_bw()
```
]

.footnote[[*]According to `PopED`]

---

class: poll-slide

# Poll Question #2

## Which of these designs will give us the best<sup>*</sup> RSE for KA, assuming a single 10 mg dose in 20 adult (70 kg) subjects?

```{r echo=FALSE, results='asis'}
ff <- function(model_switch, xt, parameters, poped.db){
  with(as.list(parameters),{
    
    CL <- CL*(WT/70)^(WT_CL)
    V <- V*(WT/70)^(WT_V)
    
    y_sd <- (DOSE * KA/(V * (KA - CL/V))) *
      (exp(-CL/V * xt) - exp(-KA * xt))
    
    y_ss <- (DOSE * KA/(V * (KA - CL/V))) *
      (exp(-CL/V * xt) / (1 - exp(-CL/V * TAU)) -
         exp(-KA * xt) / (1 - exp(-KA * TAU)))
    
    y <- xt
    y[model_switch == 1] <- y_sd[model_switch == 1]
    y[model_switch == 2] <- y_ss[model_switch == 2]
      
    return(list(y = y, poped.db = poped.db))
  })
}
fg <- function(x, a, bpop, b, bocc){
  parameters = c(
    CL    = bpop[1] * exp(b[1]),
    V     = bpop[2] * exp(b[2]),
    KA    = bpop[3] * exp(b[3]),
    WT_CL = bpop[4],
    WT_V  = bpop[5],
    DOSE  = a[1] * 1000,
    TAU   = a[2],
    WT    = a[3]
  )
  return(parameters) 
}
poped_db <- create.poped.database(
  ff_fun = ff,
  fg_fun = fg,
  fError_fun = feps.add.prop,
  bpop = c(CL = 10, V = 100, KA = 0.25, WT_CL = 0.75, WT_V = 1), 
  notfixed_bpop = c(1, 1, 1, 0, 0),
  d = c(CL = 0.08, V = 0.1, KA = 0.2), 
  sigma = c(0.05, 1),
  m = 1,
  groupsize = 10,
  xt = c(1, 24, 48, 96),
  minxt = c(0, 24, 48, 96),
  maxxt = c(12, 24, 48, 96),
  model_switch = rep(1, 4),
  a = cbind(DOSE = 10, TAU = 24, WT = 70)
)
map_chr(unique(designs[["design"]]), function(.design) {
  tmp_xt <- designs %>% 
    dplyr::filter(design == .design) %>% 
    pull(xt)
  tmp_poped_db <- create.poped.database(
    poped_db,
    xt = tmp_xt
  )
  rse <- round(unname(get_rse(evaluate.fim(tmp_poped_db), tmp_poped_db)[1]), 1)
  return(glue("* {.design} (first sample at {tmp_xt[1]} hours): RSE = {rse}%\n\n"))
}) %>% 
  writeLines()
```

.footnote[[*]According to `PopED`]
---

# The `PopED` setup

`PopED` requires 3 functions in order to define a model:

* `ff()`, the structural model;
* `fg()`, the parameter model (including IIV and IOV);
* `feps()`, the residual error model.

(The names of the functions can be different, but these are the naming conventions used by `PopED`.)  There are built-in `ff()` and `feps()` functions for basic structural models with additive and/or proportional residual error, but you'll need to at least write your own `fg()` function (don't worry, there are examples to get you started).

---

# `ff()`

`PopED` expects a function with the following arguments:

* `model_switch`: A vector of values, the same size as `xt`, identifying which model response should be computed for the corresponding `xt` value (e.g., for models with PK and PD responses).
* `xt`: A vector of independent variable values (often time).
* `parameters`: A named list of parameter values.
* `poped.db`: A `PopED` database. This can be used to extract information that may be needed in the model file.

We define a model with single dose and steady-state components, making use of the `model_switch` argument to determine which expression to use at each timepoint.

```{r}
ff <- function(model_switch, xt, parameters, poped.db){
  with(as.list(parameters),{
    
    CL <- CL*(WT/70)^(WT_CL)
    V <- V*(WT/70)^(WT_V)
    
    y_sd <- (DOSE * KA/(V * (KA - CL/V))) *
      (exp(-CL/V * xt) - exp(-KA * xt))
    
    y_ss <- (DOSE * KA/(V * (KA - CL/V))) *
      (exp(-CL/V * xt) / (1 - exp(-CL/V * TAU)) -
         exp(-KA * xt) / (1 - exp(-KA * TAU)))
    
    y <- xt
    y[model_switch == 1] <- y_sd[model_switch == 1]
    y[model_switch == 2] <- y_ss[model_switch == 2]
      
    return(list(y = y, poped.db = poped.db))
  })
}
```

---

# `fg()`

Next is the parameter model, where we add IIV and/or IOV.  Again, there are several arguments that `PopED` expects:

* `x`: A vector of discrete design variables (not used here).
* `a`: A vector of covariates. Note that dose and dosing interval are also passed in as covariates, in addition to what we'd normally classify as covariates in a PK/PD model.
* `bpop`: A vector of fixed effect parameters (i.e., `THETA`s).
* `b`: A vector of individual IIV random effects (i.e., `ETA`s).
* `bocc`: A vector of individual IOV random effects (i.e., `ETA`s) (not used here).

In this example, we include IIV on `CL`, `V`, and `KA`, and pass through dose, tau, and body weight as covariates.

```{r}
fg <- function(x, a, bpop, b, bocc){
  parameters = c(
    CL    = bpop[1] * exp(b[1]),
    V     = bpop[2] * exp(b[2]),
    KA    = bpop[3] * exp(b[3]),
    WT_CL = bpop[4],
    WT_V  = bpop[5],
    DOSE  = a[1] * 1000,
    TAU   = a[2],
    WT    = a[3]
  )
  return(parameters) 
}
```

---

# `feps()`

Finally, we define the residual error model structure.  We're using one of the built-in functions, but suppose instead we wanted a log-normal residual error model (i.e., additive on the log scale). We would need to define a custom function for this as well.  The setup is a bit esoteric, so we would just start with one of the built-in functions and tweak as necessary.  There's only one new argument here:

* `epsi`: A matrix of residual random effects (i.e. `EPS`s or `ERR`s).

```{r}
feps <- function(model_switch, xt, parameters, epsi, poped.db){
  returnArgs <- do.call(
    poped.db$model$ff_pointer,
    list(model_switch, xt, parameters, poped.db)
  ) 
  y <- returnArgs[[1]]
  poped.db <- returnArgs[[2]]
  y = y * exp(epsi[, 1])
  return(list(y = y, poped.db = poped.db)) 
}
```

---

# `create.poped.database()`

Now that we have our model defined, we bring it all together with details of the design.  There's a lot going on here even for this simple example (the documentation is 13 pages long for this function alone), so we'll break this down into pieces.

```{r}
poped_db <- create.poped.database(
  ff_fun = ff,
  fg_fun = fg,
  fError_fun = feps.add.prop,
  bpop = c(CL = 10, V = 100, KA = 0.25, WT_CL = 0.75, WT_V = 1), 
  notfixed_bpop = c(1, 1, 1, 0, 0),
  d = c(CL = 0.08, V = 0.1, KA = 0.2), 
  sigma = c(0.05, 1),
  m = 1,
  groupsize = 12,
  xt = c(5, c(rep(24, 3), 168)),
  minxt = c(0, c(rep(23, 3), 96)),
  maxxt = c(6, c(rep(24, 3), 168)),
  model_switch = c(1, rep(2, 4)),
  a = cbind(DOSE = 10, TAU = 24, WT = 32)
)
```

We've covered the functions used in the first 3 arguments.  Let's break down the rest.

```{r eval=FALSE}
  bpop = c(CL = 10, V = 100, KA = 0.25, WT_CL = 0.75, WT_V = 1), 
```

These are our current best estimates of the fixed effect parameters (`THETA`s).

```{r eval=FALSE}
  notfixed_bpop = c(1, 1, 1, 0, 0),
```

For this example, we're assuming that our weight covariate parameters are fixed in the model.  We need to tell `PopED` they're not not fixed (yes, that's a double negative).  Each element of this vector corresponds to an element of `bpop`.  We use `1` to denote "not fixed" (estimated) and `0` to denote "not not fixed" (not estimated).

```{r eval=FALSE}
  d = c(CL = 0.08, V = 0.1, KA = 0.2), 
```

These are the diagonal elements of the IIV covariance matrix (`OMEGA`).  `covd` could be used for off-diagonal elements, but we're assuming those are all zero here.

```{r eval=FALSE}
  sigma = c(0.05, 1),
```

Diagonal elements of the residual covariance matrix.

```{r eval=FALSE}
  m = 1,
  groupsize = 12,
```

`m` is the number of groups, with `groupsize` subjects in each group.  We could use multiple groups to define multiple arms of a study, or to assign different designs to different subjects.

```{r eval=FALSE}
  xt = c(5, c(rep(24, 3), 168)),
```

Our initial sampling design.

```{r eval=FALSE}
  minxt = c(0, c(rep(23, 3), 96)),
  maxxt = c(6, c(rep(24, 3), 168)),
```

These define the design space for our sampling times.  When optimizing, potential sampling times will be evaluated between these bounds.

```{r eval=FALSE}
  model_switch = c(1, rep(2, 4)),
```

The way we've defined our `ff()` function above, this allows us to signify that the first sampling time is after the first dose (`model_switch == 1`), and the remaining 4 samples are at steady state (`model_switch == 2`).

```{r eval=FALSE}
  a = cbind(DOSE = 10, TAU = 24, WT = 32)
```

The "covariates" in our model.  Note that we're using a single body weight here to keep things simple, but we could [use a distribution if necessary](https://cran.r-project.org/web/packages/PopED/vignettes/examples.html#distribution-of-covariates).

---

# Test plot

`PopED` includes a function to generate a quick test plot showing the typical response(s), along with the initial sampling times.

```{r}
plot_model_prediction(
  poped_db,
  model.names = c("Day 1", "Steady state"),
  facet_scales = "free_x",
  model_num_points = 200
) +
  labs(x = "Time from dose (h)") +
  theme_bw()
```

---

# Evaluate FIM

```{r}
FIM <- evaluate.fim(poped_db) 
det(FIM)
```

This determinant is what will be used to optimize the design, but it's not particularly helpful by itself.  What we really need are the predicted standard errors based on the FIM.

```{r}
get_rse(FIM, poped_db)
```

The RSEs are crazy high, which suggests that the model is not identifiable.  A slight tweak to a single timepoint (increasing the number of support points) may help.

```{r}
poped_db2 <- create.poped.database(
  poped_db,
  xt = c(5, c(23, 24, 24, 168))
)
FIM2 <- evaluate.fim(poped_db2) 
get_rse(FIM2, poped_db2)
```

The RSEs are no longer in the millions, but certainly not as low as we'd hope for. Let's see if we can make any improvements with optimization.

---

# *D*-optimal design

## Starting from the original design

```{r}
output <- readRDS("../script/opt1.rds")
summary(output)
```

Better, but still not good enough.

---

# *D*-optimal design

## Adding another post dose sample at steady state

Add another post dose sample at steady state.

We'll create the PopED database from scratch again.  This is probably easier than updating an old database when you change the number of samples because certain vectors (like `grouped_xt`) are generated automatically if you don't supply the argument and we don't want to fuss with that.

```{r}
poped_db_extra_ss <- create.poped.database(
  ff_fun = ff,
  fg_fun = fg,
  fError_fun = feps.add.prop,
  bpop = c(CL = 10, V = 100, KA = 0.25, WT_CL = 0.75, WT_V = 1), 
  notfixed_bpop = c(1, 1, 1, 0, 0),
  d = c(CL = 0.08, V = 0.1, KA = 0.2), 
  sigma = c(0.05, 1),
  m = 1,
  groupsize = 12,
  xt = c(5, c(rep(24, 3), 4, 168)),
  minxt = c(0, c(rep(23, 3), 0, 96)),
  maxxt = c(6, c(rep(24, 3), 6, 168)),
  model_switch = c(1, rep(2, 5)),
  a = cbind(DOSE = 10, TAU = 24, WT = 32)
)
```

```{r}
FIM_extra_ss <- evaluate.fim(poped_db_extra_ss) 
get_rse(FIM_extra_ss, poped_db_extra_ss)
```

```{r}
output_extra_ss <- readRDS("../script/opt2.rds")
summary(output_extra_ss)
```

---

# *D*-optimal design

## Adding sample after the final (steady-state) dose

Add another sample after the final dose.

This time we can just update the previous PopED database.

```{r}
poped_db_final <- create.poped.database(
  poped_db_extra_ss,
  xt = c(5, c(rep(24, 3), 72, 168)),
  minxt = c(0, c(rep(23, 3), 0, 168)),
  maxxt = c(6, c(rep(24, 3), 168, 168))
)
```

```{r}
FIM_final <- evaluate.fim(poped_db_final) 
get_rse(FIM_final, poped_db_final)
```

```{r}
output_final <- readRDS("../script/opt3.rds")
summary(output_final)
```

---

# Near-optimal design

This optimal design serves us well, but it's not very practical to request a sample at 41.09 hours after the final dose.  For a dose at 8 AM, that would mean a sample at around 1 AM.  Instead we'll construct a similar design with more practical times and see how that affects our RSEs.  We can do this by building a new `PopED` database based on the previous one.

```{r}
poped_db_practical <- create.poped.database(
  poped_db_final,
  xt = c(0.5, rep(24, 3), 32, 168)
)
FIM_practical <- evaluate.fim(poped_db_practical) 
get_rse(FIM_practical, poped_db_practical)
plot_model_prediction(
  poped_db_practical,
  model.names = c("Day 1", "Steady state"),
  facet_scales = "free_x",
  model_num_points = 200
) +
  labs(x = "Time from dose (h)") +
  theme_bw()
```

This second-to-last sample at 32 hours would be at 4 PM the day after an 8 AM final dose.  More practical, and we don't seem to lose too much in terms of RSE.

---

# Sampling windows

Next, we'll construct windows around our sampling times to provide some flexibility in sampling.  We evaluate the effect of this flexibility on the RSEs.

We'll allow:

* up to 15 minutes before or after the first sample at half an hour (i.e., 15-45 minutes post dose)
* up to 1 hour before each trough sample (i.e., 0-1 hours pre dose)
* up to 2 hours before or after the 32-hour sample after the final dose (i.e., 30-34 hours after the final dose)
* up to 4 hours before the final sample (i.e., 164-168 hours post final dose)

```{r}
plot_efficiency_of_windows(
  poped_db_practical,
  xt_plus  = c(0.25, rep(0, 3), 2, 0),
  xt_minus = c(0.25, rep(1, 3), 2, 4)
)
```

The 100 sets of simulated samples show no significant deviations from the RSEs for the optimal samples.

---

class: inverse, center, middle

# Example: ODE PK model

---
class: inverse, center, middle

# Example: ODE PK/PD model

---

# References

```{r refs, echo=FALSE, results="asis"}
PrintBibliography(myBib)
```

---

https://www.flickr.com/photos/hyperxp/6847495120
https://www.flickr.com/photos/dmuth/6039404494
https://www.flickr.com/photos/dmuth/6039404304

https://www.flickr.com/photos/dok1/3581901453
https://www.flickr.com/photos/bucky1105/2334669404
https://www.flickr.com/photos/lalonsorm/16856604020
https://www.flickr.com/photos/gmanviz/48835009102
